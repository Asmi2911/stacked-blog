---
title: 'Beyond Accuracy: Evaluating LLMs for Faithfulness, Retrieval, and Real-World Performance'
date: '2025-01-15'
lastmod: '2025-01-15'
tags: ['AI', 'evaluation', 'ML-systems', 'engineering']
draft: false
summary: 'A practical framework for evaluating LLM applications — measuring what truly matters: faithfulness, retrieval quality, and cost-per-correct in production systems.'
images: ['/static/images/llm1.png']
---

## Introduction

Accuracy has long been the north star of model evaluation but in production, **accuracy alone tells an incomplete story**.  
When models power real workflows, the questions shift from _"Is it right?"_ to _"Is it reliable, faithful, and efficient?"_

This post distils lessons from large-scale deployments into a practical framework for evaluating **LLM systems as systems** — not just isolated models.

---

## The Dimensions that Matter

![Eval dimensions](/static/images/llm2.webp)

A robust evaluation framework moves beyond raw accuracy to include four intertwined pillars:

1. **Faithfulness** — Does the model’s response remain grounded in the provided evidence?  
   Penalise hallucinations and measure factual consistency against references.

2. **Retrieval Quality** — Does the retriever surface the right context?  
   Evaluate document recall and coverage against a gold standard.

3. **Latency Budget** — How fast can it respond without compromising reliability?  
   Record P50 and P95 latency across stages: retrieval, generation, and post-processing.

4. **Cost-Per-Correct (CPC)** — How much does a correct answer _actually_ cost?  
   Track total token expenditure per verified correct response.

Together, these metrics reveal trade-offs between **truth, speed, and cost**, giving engineers the levers to optimise all three instead of one.

---

## Building a Practical CI Harness

A minimal evaluation loop can be integrated directly into your CI/CD pipeline:

1. Label a small test set of questions, gold answers, and reference documents.
2. Compute **faithfulness** using entailment or citation checks.
3. Measure **retrieval recall** versus expected sources.
4. Log **latency and CPC** metrics at every deployment.
5. Block merges that fall below threshold — e.g. Faithfulness ≥ 0.9, CPC ≤ target.

This transforms evaluation from a research afterthought into a **governance layer**, a living quality gate that ensures models remain aligned, affordable, and consistent.

---

## Reflection

An LLM that’s merely “accurate” may still mislead, waste tokens, or fail under load.  
An LLM that’s _faithful, efficient, and measurable_ is production-ready.

In the end, it’s not about chasing higher accuracy —  
it’s about building **trustworthy systems** that earn it.
